{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents:\n",
    "* [LangChain](#langchain)\n",
    "* [LLaMA2](#LLaMA2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wptzjSY9Cq5Y",
    "outputId": "749175f6-6815-438c-9641-709da205ea3a"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fW9XSdx57kFx",
    "outputId": "be8d1df9-0780-4a33-fa81-3a706979ce8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\dwtro\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token hf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBnK4mR0C9yY",
    "outputId": "82cb23f1-2d5c-419f-f6d2-7bc948729c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 61.4/991.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 450.6/991.5 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 7.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stanza) (4.25.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stanza) (3.1)\n",
      "Collecting toml (from stanza)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stanza) (2.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from stanza) (4.66.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from torch>=1.3.0->stanza) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from requests->stanza) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from requests->stanza) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Downloading stanza-1.8.1-py3-none-any.whl (970 kB)\n",
      "   ---------------------------------------- 0.0/970.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 174.1/970.4 kB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 614.4/970.4 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 970.4/970.4 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 433.8/433.8 kB 13.2 MB/s eta 0:00:00\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: toml, emoji, stanza\n",
      "Successfully installed emoji-2.11.0 stanza-1.8.1 toml-0.10.2\n",
      "Requirement already satisfied: tqdm in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dwtro\\anaconda3\\envs\\dp_lrn\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade stanza\n",
    "!pip install --upgrade tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run script server troubleshoot\n",
      "trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "\n",
    "It enables applications that:\n",
    "\n",
    "Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
    "\n",
    "Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "98fa0acba3614fd185158ec7e8035087",
      "b8232e95aaf4447c80d609694e51abec",
      "07aa70e5250f4d37873b5035805a036d",
      "99f89bd3af604472a4de2ae36bc8ea01",
      "4bf7d1edf8754e30800c7f09b7b97feb",
      "690b93a46aad4614a00976b0399770ad",
      "56da9a491de14544a63acf77559e73f5",
      "ef479c2dd1404f64994a13a773c53ad9",
      "7dbef44e87974bb99173fbfdc8963891",
      "3c5fc03305024585bf018f07f93adb62",
      "224ab48a1ddf4724845c2b952e81a30f",
      "2f473e68a2a843d4b79a05169dc781e8",
      "0834b8989aad40c5aa9c601c5828f337",
      "358b0d54c2d04f01bd818d00056354be",
      "df449e2c18fa454099f44aab4c6760f8",
      "70927454b7fa484496ce4f75fa1f9f05",
      "496cff42883c47faa171d1a8b5fc2591",
      "512ffbd7223f440791c3870ffba99a92",
      "6cc27b652c2243e78dbde7b79b603505",
      "d2d99dcc5d9d45a7a6168b44a56db17f",
      "ae4dc1d86da1454991584cba305cc27e",
      "4ac20c6ce51c46dc94909967e19ec96a",
      "72a9eed60f4a43cba874ff68cd18cfc2",
      "0c2e73b374c1483384c8b6e76fccf8e9",
      "8a0a335b38fc4e03b8d48ce159fa9bc1",
      "067977965bc846c3b0597ee8b7af8b06",
      "0f001e99117c48b383d73cb5f94a87d8",
      "64336b9f28b044ab872c26a596feb595",
      "099d98799a924162b1a5e3e4fe4571c7",
      "31df09de8fe14b80b6bef34c4117a723",
      "a07e9f012353491a9827cff89afaf820",
      "ca4e31fffe9745329fcdc41b1fd108ce",
      "0fe24c83bdda4b5c970cd1d4bd47adcc",
      "6901acd88c3b4e769a1d0d9d65eb48d5",
      "0dd9f41d9dc94e93a90c276c7f260adc",
      "8c4f8294dc934729b91330760ff48412",
      "0f0ffdb600f4422d972201ea5c032981",
      "0acc5d81fc494da8b8adbd23a3be7f1c",
      "610a3e60758946e8b1b802e83814f303",
      "5844fd32cd2b48c3b2c8bceedd4c4526",
      "69989e37c0c944f7a0e9a8a8869618cf",
      "6645466776b84ad581e15b405c9dcef5",
      "c9c2cf0822cc4deaae4ce335f92fe87d",
      "6d984b9f4b8a4d7fb56297b8f3f7d8f0",
      "f2f54729ba6b4dfe83dba4b754900997",
      "53e82075c7294c34be07a38fffe9153c",
      "c2a5b9c060f847ed9aaf44b9d9fda9a1",
      "18ef9e57d7c347c9bbf6c0f4318f3dc8",
      "318c871279194a8889a83ee6c96480ce",
      "5d0b1365e1af47f2bce3ba92130fb941",
      "73c1b2cbe49c489d9767e022d3bfa63a",
      "0ca06e9c281e44b49a85b509140f8f67",
      "bebd092e518c42eca46cafebc70e511d",
      "cd97698478ec43ab844bc99fdc609969",
      "3b307414d8774dbbb67059e6e1fe342d",
      "99cb52c257d3421fa9e015b7e2cca343",
      "a4687633928b428f9d951bb4e1d90481",
      "053a784a1408476287d0f0d6dcee72f1",
      "80f7c9ad9fe048c29d99c59934ec8a20",
      "b8a83a4a43b5485f82128dc614fd0beb",
      "b37e5092760b4222b7bcaa960eb83ac1",
      "66056c636e86481a8f8c2a1123ca10ad",
      "6739b139f65c42b78ac7d54068590f6a",
      "06cb9fb38fc146848164c70ae0224681",
      "4354aff0371945d4a821ea9e86a8715b",
      "5263c6973d844f99b4d6f89daff6bf09",
      "3fc93fef3e6e4bc7898a8f5ada8f3278",
      "3a6fd77d1ec7465db8d8f6c43ee0a25e",
      "c914fe560fca4c98ac741d1ad3149c34",
      "03a54b63b8474ea794a942f3b00230ea",
      "436acf47aab446fda65c5db39d88dad1",
      "af99e66efd7c419f963d6e9375c9ebab",
      "dd9a4e8bd2a24164a53ee8c1890ffe27",
      "5229a43c55644f96a60dc56421d372c9",
      "a7b92acd09b842c4848042b8a93759d0",
      "761042de6e6d48c0abcc8a71051e2e6e",
      "221acbf6953e4f5796ae7f89f0d9ded2",
      "1c15529d2640491faf30f8cfeb5874e1",
      "8936c24718604d99b49c252825dcb06b",
      "f737cfe17fbc4cceaaef56e043e777b9",
      "507f6e43ac7e4eb38ddd0a0a4f8fe900",
      "49884f2eb52440b881ddec3d4db8e234",
      "14413654a23344c88ca89cafa5496842",
      "25b18327d4714d0c8d7743bc5ccbfb34",
      "1ed3d9b0e9a0404793b6a452ee1365e6",
      "9199b9e1ec714f619dad9756171e3918",
      "526689467206491d9a1e84c53797b19e",
      "317b4926ac1e44c6a311f23e61a474a2",
      "8c9a94c781d64faeab1f8cc5019d69a2",
      "d672fab2343d42f0b40b0d8068686492",
      "24fde3f4ff794e318c4d05a053b59eec",
      "b2ce30396b16425ab5e0b08b4ed89c9e",
      "19e2c2fa3538493599c88b31ac0aae3e",
      "6435cec4167e4fe2bc432cb2af9ef43f",
      "b57678f4ea9b4f8db42b95a280d0b6c1",
      "ed1f0890b34141d9ba314b9a68acb0bf",
      "afa1c3b2290a41c2b6fdd813b1e5dfb1",
      "4f0e9df8b1c247129a3f5ea51dac5733",
      "637f471c572f4b4190fd17b6cf12c45d",
      "d56500c1f67e4ceeb328120d1593ac58",
      "c4d431cc88c343d7903bbb9cfc06cedf",
      "30050553cb1040788d42b8aee9af7217",
      "74b3e3c72ba1447ebb294145e13773a3",
      "e508926535bb40a8bd8838035d48a733",
      "2ef3d1ac7274429aa3250cd4cfdd75de",
      "36f0590fa71c4e6aa732b7c4bdb7100a",
      "4d85e4ac4a88434c87fc7b79e5939e8f",
      "79304f97b164465cac3109a666176425",
      "1d092c72d4f34b09b1a9a2b1d97e3bed",
      "019519f1dfb242f4bb9403438e743464",
      "0c35adaf06f6461db75b93c2ec9df5b9",
      "fdb48d2776664d338ab5737198aa454e",
      "0b2c69796ce44add9d9c394535862093",
      "38175015bd0a432f9e9c4f060da14814",
      "7360aaddb39a45f69e60857454826988",
      "9ba152dab02c4b16ad49b7376232a833",
      "5b692e68b98149ef9c53050e7360a71a",
      "347ed9043e8449d79ed3ffedd03dbeba",
      "56ea276171d14b44b314426de50b4c35",
      "d6382ff48d6c4cb6bce3f6957e0e9b3b",
      "73736eda7c6842899807a6da369b08c2"
     ]
    },
    "id": "SNu9GNwlCr1R",
    "outputId": "4fd004b5-7065-41bd-c1cd-c6aac7eb2805"
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch, io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "#f = io.StringIO()\n",
    "#with redirect_stdout(f)   \n",
    "\n",
    "#from contextlib import redirect_stdout\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline( #defining the Pipeline\n",
    "\"text-generation\", #task\n",
    "model=model,\n",
    "tokenizer=tokenizer,\n",
    "torch_dtype=torch.bfloat16,\n",
    "trust_remote_code=True,\n",
    "device_map=\"auto\",\n",
    "max_length=1000,\n",
    "do_sample=True,\n",
    "top_k=10,\n",
    "num_return_sequences=1,\n",
    "eos_token_id=tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LL7JGQ5iCzIy"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RN_t-Td3C0Aq",
    "outputId": "1e6f196f-6f2a-41a9-9a58-faabd3bbba89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • Meta is publicly releasing LLaMA, a state-of-the-art foundational large language model designed to help researchers advance their work in the field of AI.\n",
      "            • LLaMA is a smaller, more performant model that enables others in\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,  LLMChain #chaining to run a query against LLMs chain = prompt|llm\\parser\n",
    "\n",
    "template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "           \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm) # to run a query against LLMs\n",
    "\n",
    "text = \"\"\" As part of Meta’s commitment to open science, today we are publicly releasing LLaMA (Large Language Model Meta AI), a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI. Smaller, more performant models such as LLaMA enable others in the research community who don’t have access to large amounts of infrastructure to study these models, further democratizing access in this important, fast-changing field.\n",
    "\n",
    "Training smaller foundation models like LLaMA is desirable in the large language model space because it requires far less computing power and resources to test new approaches, validate others’ work, and explore new use cases. Foundation models train on a large set of unlabeled data, which makes them ideal for fine-tuning for a variety of tasks. We are making LLaMA available at several sizes (7B, 13B, 33B, and 65B parameters) and also sharing a LLaMA model card that details how we built the model in keeping with our approach to Responsible AI practices.\n",
    "\n",
    "Over the last year, large language models — natural language processing (NLP) systems with billions of parameters — have shown new capabilities to generate creative text, solve mathematical theorems, predict protein structures, answer reading comprehension questions, and more. They are one of the clearest cases of the substantial potential benefits AI can offer at scale to billions of people.\n",
    "\n",
    "Even with all the recent advancements in large language models, full research access to them remains limited because of the resources that are required to train and run such large models. This restricted access has limited researchers’ ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues, such as bias, toxicity, and the potential for generating misinformation.\n",
    "\n",
    "Smaller models trained on more tokens — which are pieces of words — are easier to retrain and fine-tune for specific potential product use cases. We trained LLaMA 65B and LLaMA 33B on 1.4 trillion tokens. Our smallest model, LLaMA 7B, is trained on one trillion tokens.\n",
    "\n",
    "Like other large language models, LLaMA works by taking a sequence of words as an input and predicts a next word to recursively generate text. To train our model, we chose text from the 20 languages with the most speakers, focusing on those with Latin and Cyrillic alphabets.\n",
    "\n",
    "There is still more research that needs to be done to address the risks of bias, toxic comments, and hallucinations in large language models. Like other models, LLaMA shares these challenges. As a foundation model, LLaMA is designed to be versatile and can be applied to many different use cases, versus a fine-tuned model that is designed for a specific task. By sharing the code for LLaMA, other researchers can more easily test new approaches to limiting or eliminating these problems in large language models. We also provide in the paper a set of evaluations on benchmarks evaluating model biases and toxicity to show the model’s limitations and to support further research in this crucial area.\n",
    "\n",
    "To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license focused on research use cases. Access to the model will be granted on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world. People interested in applying for access can find the link to the application in our research paper.\n",
    "\n",
    "We believe that the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear guidelines around responsible AI in general and responsible large language models in particular. We look forward to seeing what the community can learn — and eventually build — using LLaMA.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMht9XnjGAou",
    "outputId": "c40a994c-fec9-42d8-c825-8b09a041408d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " • Tesla, Inc. is an American multinational automotive and clean energy company.\n",
      "            • Tesla designs and manufactures electric vehicles, stationary battery energy storage devices, solar panels, and solar roof tiles.\n",
      "            • Tesla is one of the world's most valuable companies and led the battery electric vehicle market in 2022.\n",
      "            • Tesla Energy develops and installs photovoltaic systems in the United States and is a major supplier of battery energy storage systems.\n",
      "            • Elon Musk became CEO in 2008 and the company's mission is to expedite the move to sustainable transport and energy.\n",
      "            • Tesla has produced several car models, including the Roadster, Model S, Model X, Model 3, Model Y, and Cybertruck.\n",
      "            • Tesla's 2022 deliveries were around 1.31 million vehicles, a 40% increase over the previous year, and cumulative sales totaled 4 million cars as of April 2023.\n",
      "            • Tesla has faced lawsuits, government scrutiny, and journalistic criticism\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "Tesla, Inc. (/ˈtɛslə/ TESS-lə or /ˈtɛzlə/ TEZ-lə[a]) is an American multinational automotive and clean energy company headquartered in Austin, Texas. Tesla designs and manufactures electric vehicles (cars and trucks), stationary battery energy storage devices from home to grid-scale, solar panels and solar roof tiles, and related products and services.\n",
    "\n",
    "Tesla is one of the world's most valuable companies and, as of 2023, was the world's most valuable automaker. In 2022, the company led the battery electric vehicle market, with 18% share.\n",
    "\n",
    "Its subsidiary Tesla Energy develops and is a major installer of photovoltaic systems in the United States. Tesla Energy is one of the largest global suppliers of battery energy storage systems, with 6.5 gigawatt-hours (GWh) installed in 2022.\n",
    "\n",
    "Tesla was incorporated in July 2003 by Martin Eberhard and Marc Tarpenning as Tesla Motors. The company's name is a tribute to inventor and electrical engineer Nikola Tesla. In February 2004, via a $6.5 million investment, Elon Musk became the company's largest shareholder. He became CEO in 2008. Tesla's announced mission is to help expedite the move to sustainable transport and energy, obtained through electric vehicles and solar power.\n",
    "\n",
    "Tesla began production of its first car model, the Roadster sports car, in 2008. This was followed by the Model S sedan in 2012, the Model X SUV in 2015, the Model 3 sedan in 2017, the Model Y crossover in 2020, and the Tesla Semi truck in 2022. The company plans production of the Cybertruck light-duty pickup truck in 2023.[8] The Model 3 is the all-time bestselling plug-in electric car worldwide, and in June 2021 became the first electric car to sell 1 million units globally.[9] Tesla's 2022 deliveries were around 1.31 million vehicles, a 40% increase over the previous year,[10][11] and cumulative sales totaled 4 million cars as of April 2023.[12] In October 2021, Tesla's market capitalization temporarily reached $1 trillion, the sixth company to do so in U.S. history.\n",
    "\n",
    "Tesla has been the subject of lawsuits, government scrutiny, and journalistic criticism, stemming from allegations of whistleblower retaliation, worker rights violations, product defects, and Musk's many controversial statements.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_chain.run(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GbGQyZJLn2o",
    "outputId": "2666c3cf-9f4c-4b81-d444-be223e316d58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Apple Inc. is an American multinational technology company\n",
      "            - Apple is the world's largest technology company by revenue\n",
      "            - Apple is the world's biggest company by market capitalization\n",
      "            - Apple was founded in 1976 by Steve Wozniak, Steve Jobs, and Ronald Wayne\n",
      "            - Apple went public in 1980 to instant financial success\n",
      "            - Apple lost market share to Microsoft Windows operating system on Intel-powered PC clones in the 1990s\n",
      "            - Apple bought NeXT in 1997 to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company\n",
      "            - Tim Cook succeeded Jobs as CEO in 2011 after Jobs resigned due to health reasons\n",
      "            - Apple became the first publicly traded U.S. company to be\n"
     ]
    }
   ],
   "source": [
    "text2 =\"\"\"\n",
    "Apple Inc. is an American multinational technology company headquartered in Cupertino, California. Apple is the world's largest technology company by revenue, with US$394.3 billion in 2022 revenue.[6] As of March 2023, Apple is the world's biggest company by market capitalization.[7] As of June 2022, Apple is the fourth-largest personal computer vendor by unit sales and the second-largest mobile phone manufacturer in the world. It is often considered as one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Meta Platforms, and Microsoft.\n",
    "\n",
    "Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs (1955–2011) and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers. Apple went public in 1980 to instant financial success. The company developed computers featuring innovative graphical user interfaces, including the 1984 original Macintosh, announced that year in a critically acclaimed advertisement called \"1984\". By 1985, the high cost of its products, and power struggles between executives, caused problems. Wozniak stepped back from Apple and pursued other ventures, while Jobs resigned and founded NeXT, taking some Apple employees with him.\n",
    "\n",
    "As the market for personal computers expanded and evolved throughout the 1990s, Apple lost considerable market share to the lower-priced duopoly of the Microsoft Windows operating system on Intel-powered PC clones (also known as \"Wintel\"). In 1997, weeks away from bankruptcy, the company bought NeXT to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company. Over the next decade, Jobs guided Apple back to profitability through a number of tactics including introducing the iMac, iPod, iPhone and iPad to critical acclaim, launching the \"Think different\" campaign and other memorable advertising campaigns, opening the Apple Store retail chain, and acquiring numerous companies to broaden the company's product portfolio. When Jobs resigned in 2011 for health reasons, and died two months later, he was succeeded as CEO by Tim Cook.\n",
    "\n",
    "Apple became the first publicly traded U.S. company to be valued at over $1 trillion in August 2018, then at $2 trillion in August 2020, and at $3 trillion in January 2022. As of April 2023, it was valued at around $2.6 trillion. The company receives criticism regarding the labor practices of its contractors, its environmental practices, and its business ethics, including anti-competitive practices and materials sourcing. Nevertheless, the company has a large following and enjoys a high level of brand loyalty. It has also been consistently ranked as one of the world's most valuable brands.\n",
    "\"\"\"\n",
    "print(llm_chain.run(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Humanity is the state or quality of being human.\n",
      "            - It is a complex and multifaceted concept that encompasses a wide range of characteristics and experiences.\n",
      "            - Humanity is not limited to any one culture, race, or ethnicity, but is a universal aspect of the human experience.\n",
      "            - Understanding and appreciating the diversity of humanity is essential for building a more inclusive and equitable society.\n",
      "            - The concept of humanity is constantly evolving and adapting to changing social, cultural, and technological contexts.\n",
      "            - The future of humanity is uncertain and may be shaped by a variety of factors, including scientific and technological advancements, political and economic systems, and environmental factors.\n",
      "            - The preservation of humanity is essential for ensuring the continued survival and well-being of future generations.\n",
      "            - The concept of humanity is closely tied to the idea of dignity and worth, and is essential for promoting social justice and human rights.\n",
      "            - The appreciation and celebration of humanity is essential for fostering a sense of community and cooperation among individuals and groups.\n",
      "            - The concept of humanity is not limited to the present, but also encompasses the past and future, and is shaped by historical and cultural contexts.\n",
      "            - The concept of humanity is complex and multifaceted, and requires ongoing reflection and dialogue to fully understand and appreciate its implications.\n",
      "            - The future of humanity is uncertain and may be shaped by a variety of factors, including scientific and technological advancements, political and economic systems, and environmental factors.\n",
      "            - The preservation of humanity is essential for ensuring the continued survival and well-being of future generations.\n",
      "            - The concept of humanity is closely tied to the idea of dignity and worth, and is essential for promoting social justice and human rights.\n",
      "            - The appreciation and celebration of humanity is essential for fostering a sense of community and cooperation among individuals and groups.\n",
      "            - The concept of humanity is not limited to the present, but also encompasses the past and future, and is shaped by historical and cultural contexts.\n",
      "            - The concept of humanity is complex and multifaceted, and requires ongoing reflection and dialogue to fully understand and appreciate its implications.\n",
      "         \n",
      "```\n",
      "\n",
      "I hope this summary helps you understand the key points of the text. Please let me know if you have any further questions or need any additional assistance.\n",
      " * Humanity is the state or quality of being human.\n",
      "            * It encompasses the characteristics, abilities, and experiences that define us as humans.\n",
      "            * Humanity is complex and multifaceted, involving both our individual and collective aspects.\n",
      "            * Understanding humanity is essential for building a better world for all.\n",
      "            * By exploring the depths of humanity, we can gain insights into our own nature and the nature of existence.\n",
      "            * The study of humanity can help us to develop empathy, compassion, and a deeper appreciation for the human experience.\n",
      "            * Ultimately, the pursuit of humanity is a never-ending journey of discovery and growth.\n",
      "            */\n",
      "\n",
      "I hope this summary helps you! Let me know if you have any questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "text3 = \"\"\"\n",
    "The topic is on Humanity. It should be very concise and not more than 100 words.\n",
    "\"\"\"\n",
    "print(llm_chain.run(text3))\n",
    "\n",
    "template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "           \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
